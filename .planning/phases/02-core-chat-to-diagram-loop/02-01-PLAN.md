---
phase: 02-core-chat-to-diagram-loop
plan: '01'
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/chat.ts
  - src/lib/ollama.ts
autonomous: true

must_haves:
  truths:
    - "User can type diagram request and system sends to Ollama"
    - "Ollama client handles streaming response"
    - "D2 code extracted from AI response"
  artifacts:
    - path: "src/types/chat.ts"
      provides: "Chat message type definitions"
      contains: "ChatMessage, MessageRole"
    - path: "src/lib/ollama.ts"
      provides: "Ollama API client with streaming"
      exports: ["checkOllamaHealth", "streamChat"]
---

<objective>
Create foundational Ollama client and type definitions for the chat-to-diagram loop.
</objective>

<execution_context>
@./.opencode/get-shit-done/workflows/execute-plan.md
@./.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02-core-chat-to-diagram-loop/02-RESEARCH.md

# Existing components
@src/components/ChatPanel.tsx
@src/lib/d2.ts
</context>

<tasks>

<task type="auto">
  <name>Create chat message types</name>
  <files>src/types/chat.ts</files>
  <action>
Create TypeScript types for chat messages:
- `MessageRole` type: 'user' | 'assistant' | 'system'
- `ChatMessage` interface with: id (string), role (MessageRole), content (string), d2Source (optional string), timestamp (number)
- `OllamaMessage` interface for API format: role (MessageRole), content (string)
- `ChatState` interface: messages (ChatMessage[]), currentD2 (string), isGenerating (boolean), error (string | null)
- `StreamCallbacks` interface: onChunk (function), onComplete (function), onError (function)

Export all types. Use explicit types throughout (no `any`).
  </action>
  <verify>
TypeScript compiles without errors. Run: npx tsc --noEmit
  </verify>
  <done>
src/types/chat.ts exists with ChatMessage, MessageRole, OllamaMessage, ChatState, StreamCallbacks types exported
  </done>
</task>

<task type="auto">
  <name>Implement Ollama client with streaming</name>
  <files>src/lib/ollama.ts</files>
  <action>
Create Ollama client module:

1. Constants: OLLAMA_BASE = 'http://localhost:11434', DEFAULT_MODEL = 'llama3.2', HEALTH_TIMEOUT = 5000

2. `checkOllamaHealth(): Promise<boolean>` - GET request to / with 5-second AbortSignal timeout. Returns true if response.ok, false on error.

3. `streamChat(model: string, messages: OllamaMessage[], callbacks: StreamCallbacks): Promise<string>`:
   - POST to /api/chat with { model, messages, stream: true }
   - Use response.body?.getReader() with TextDecoder
   - Parse each line as JSON, yield content from data.message?.content
   - Accumulate full response and call onComplete(fullResponse) when done
   - Handle errors by calling onError and re-throwing
   - Use AbortController for cleanup on unmount

4. `extractD2Code(response: string): { explanation: string; d2Code: string }`:
   - Match /```d2\n([\s\S]*?)```/ regex
   - Return { explanation: response without block, d2Code: matched content }
   - Fallback: if no block but response has ':' and '[' treat as D2
   - Return { explanation: response, d2Code: '' } if no D2 found

5. `formatError(error: Error | unknown): string`:
   - Format user-friendly messages for: connection refused, timeout, model not found, rate limit

Reference: @src/components/ChatPanel.tsx for message structure patterns
  </action>
  <verify>
TypeScript compiles: npx tsc --noEmit
Manual test: Start Ollama, call checkOllamaHealth(), verify returns true/false correctly
  </verify>
  <done>
src/lib/ollama.ts exports checkOllamaHealth, streamChat, extractD2Code, formatError
  </done>
</task>

</tasks>

<verification>
- [ ] src/types/chat.ts created with all type definitions
- [ ] src/lib/ollama.ts created with streaming client
- [ ] TypeScript compiles without errors
- [ ] Both files are valid ES modules
</verification>

<success_criteria>
Types and Ollama client are ready for use by chat hook in plan 02-02
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-chat-to-diagram-loop/02-01-SUMMARY.md`
</output>
